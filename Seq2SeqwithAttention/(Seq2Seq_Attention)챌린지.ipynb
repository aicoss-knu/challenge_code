{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FrameWork and Layer module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_table('./kor-eng/kor.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>가.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>안녕.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>뛰어!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run.</td>\n",
       "      <td>뛰어.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>Science fiction has undoubtedly been the inspi...</td>\n",
       "      <td>공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>I started a new blog. I'll do my best not to b...</td>\n",
       "      <td>난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>I think it's a shame that some foreign languag...</td>\n",
       "      <td>몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3853 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0                                                   Go.   \n",
       "1                                                   Hi.   \n",
       "2                                                  Run!   \n",
       "3                                                  Run.   \n",
       "4                                                  Who?   \n",
       "...                                                 ...   \n",
       "3848  Science fiction has undoubtedly been the inspi...   \n",
       "3849  I started a new blog. I'll do my best not to b...   \n",
       "3850  I think it's a shame that some foreign languag...   \n",
       "3851  If someone who doesn't know your background sa...   \n",
       "3852  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                      1  \\\n",
       "0                                                    가.   \n",
       "1                                                   안녕.   \n",
       "2                                                   뛰어!   \n",
       "3                                                   뛰어.   \n",
       "4                                                   누구?   \n",
       "...                                                 ...   \n",
       "3848       공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.   \n",
       "3849  난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...   \n",
       "3850  몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...   \n",
       "3851  만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...   \n",
       "3852  의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...   \n",
       "\n",
       "                                                      2  \n",
       "0     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "2     CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3     CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "4     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "...                                                 ...  \n",
       "3848  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "3849  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "3850  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3851  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3852  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "\n",
       "[3853 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 3763\n"
     ]
    }
   ],
   "source": [
    "#KOR\n",
    "all_data.drop_duplicates(subset = [1], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "all_data[1] = all_data[1].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "all_data[1] = all_data[1].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "all_data[1].replace('', np.nan, inplace=True, regex=True) # 공백은 Null 값으로 변경\n",
    "all_data = all_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       가\n",
       "1                                                      안녕\n",
       "2                                                      뛰어\n",
       "3                                                      뛰어\n",
       "4                                                      누구\n",
       "                              ...                        \n",
       "3848          공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어\n",
       "3849    난 블로그를 시작했어 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 않...\n",
       "3850    몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...\n",
       "3851    만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...\n",
       "3852    의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...\n",
       "Name: 1, Length: 3763, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 3470\n"
     ]
    }
   ],
   "source": [
    "#ENG\n",
    "all_data.drop_duplicates(subset = [0], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "all_data[0] = all_data[0].str.replace(\"[^a-z A-Z]\",\"\", regex=True) # 정규 표현식 수행\n",
    "all_data[0] = all_data[0].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "all_data[0].replace('', np.nan, inplace=True, regex=True) # 공백은 Null 값으로 변경\n",
    "all_data = all_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      Go\n",
       "1                                                      Hi\n",
       "2                                                     Run\n",
       "3                                                     Run\n",
       "4                                                     Who\n",
       "                              ...                        \n",
       "3848    Science fiction has undoubtedly been the inspi...\n",
       "3849    I started a new blog Ill do my best not to be ...\n",
       "3850    I think its a shame that some foreign language...\n",
       "3851    If someone who doesnt know your background say...\n",
       "3852    Doubtless there exists in this world precisely...\n",
       "Name: 0, Length: 3470, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3470/3470 [00:00<00:00, 15131.97it/s]\n"
     ]
    }
   ],
   "source": [
    "X_eng_data = []\n",
    "for sentence in tqdm(all_data[0]):\n",
    "    tokenized_sentence = [word for word in word_tokenize(sentence)] # 토큰화\n",
    "    X_eng_data.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3470/3470 [00:05<00:00, 649.99it/s] \n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "X_kor_data = []\n",
    "for sentence in tqdm(all_data[1]):\n",
    "    tokenized_sentence = [word for word in okt.morphs(sentence, stem=True)] # 토큰화\n",
    "    X_kor_data.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Word2Index(ctxt : list):\n",
    "    \n",
    "    word2index = dict()\n",
    "    words = []\n",
    "    max_len = 0\n",
    "    for text in ctxt:\n",
    "        temp_len = len(text)\n",
    "        if max_len < temp_len:\n",
    "            max_len = temp_len\n",
    "        for word_temp in text:\n",
    "            words.append(word_temp)\n",
    "    \n",
    "    word2index = {word : index+2 for index, word in enumerate(list(set(words)))}\n",
    "### word2index need <sos> and <eos> tokens\n",
    "    word2index['<sos>'] = 0\n",
    "    word2index['<eos>'] = 1  \n",
    "    \n",
    "    return word2index, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, max_len = Make_Word2Index(X_kor_data+X_eng_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, word2index): # -> input = text, word2index from above function\n",
    "    tokens = []    \n",
    "    tokenized_sentence = [word for word in okt.morphs(text, stem=True)]\n",
    "    for word in tokenized_sentence:\n",
    "        tokens.append(word2index[word])\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eng(text, word2index): # -> input = text, word2index from above function\n",
    "    \n",
    "    tokens = []\n",
    "    tokenized_sentence = [word for word in word_tokenize(text)]    \n",
    "    for word in tokenized_sentence:\n",
    "        tokens.append(word2index[word])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Make Encoder with LSTM (class) \"\"\"\n",
    "\n",
    "class Seq2Seq_Encoder_Attention(nn.Module):\n",
    "  def __init__(self, input_size, h_dim, emb_dim):\n",
    "    super(Seq2Seq_Encoder_Attention, self).__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.lstm = nn.LSTM(input_size = emb_dim, hidden_size = h_dim, batch_first = True)\n",
    "    self.drop = nn.Dropout()\n",
    "\n",
    "  def forward(self, x):\n",
    "    emb = self.drop(self.embedding(x))\n",
    "    _, (hidden_state, cell_state) = self.lstm(emb)\n",
    "    \n",
    "    return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Make Decoder with LSTM (class) \"\"\"\n",
    "\n",
    "class Seq2Seq_Decoder_Attention(nn.Module):\n",
    "  def __init__(self, output_size, h_dim, emb_dim):\n",
    "    super(Seq2Seq_Decoder_Attention, self).__init__()\n",
    "   \n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.lstm = nn.LSTM(input_size = emb_dim, hidden_size = h_dim, batch_first = True)\n",
    "    self.drop = nn.Dropout()\n",
    "    self.fc = nn.Linear(h_dim, output_size)\n",
    "\n",
    "  def forward(self, x, hidden_state, cell_state):\n",
    "    emb = self.drop(self.embedding(x))\n",
    "    emb = emb.unsqueeze(1)\n",
    "\n",
    "    result,(hidden_state, cell_state) = self.lstm(emb,\\\n",
    "                              (hidden_state, cell_state))\n",
    "    result = self.fc(result)\n",
    "    \n",
    "    return result, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Make Seq2Seq with Attention(LSTM) class \"\"\"\n",
    "\n",
    "class Seq2Seq_Attention(nn.Module):\n",
    "    def __init__(self, input_size, output_size, h_dim, emb_dim, teacher_forcing=0.5):\n",
    "        super(Seq2Seq_Attention,self).__init__()\n",
    "    \n",
    "        self.output_size = output_size\n",
    "        self.encoder = Seq2Seq_Encoder_Attention(input_size, h_dim, emb_dim)\n",
    "        self.decoder = Seq2Seq_Decoder_Attention(output_size, h_dim, emb_dim)\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.fc = nn.Linear(2*h_dim, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        hidden_state, cell_state = self.encoder(src)\n",
    "        encoder_hidden_state = hidden_state\n",
    "\n",
    "        tgt_len = tgt.size(1)\n",
    "        result = torch.zeros((tgt.size(0), tgt_len-1, self.output_size))\n",
    "        de_input = tgt[:,0]\n",
    "\n",
    "        for i in range(1, tgt_len):\n",
    "            output, hidden_state, cell_state = self.decoder(de_input, hidden_state, cell_state)\n",
    "            attn_score = self.attention_scoring(encoder_hidden_state, hidden_state)\n",
    "            weighted_sum = torch.sum(encoder_hidden_state * attn_score, dim =1)\n",
    "            hidden_state_concat = torch.cat([hidden_state, weighted_sum.unsqueeze(1)], dim = 2)\n",
    "            output = self.fc(hidden_state_concat)\n",
    "            result[:,i-1,:] = output\n",
    "            top1 = output.argmax(2)\n",
    "      \n",
    "            if self.teacher_forcing > random.random() :\n",
    "                de_input = torch.LongTensor([top1])\n",
    "                de_input = de_input.cuda()\n",
    "            else:\n",
    "                de_input = tgt[:,i]\n",
    "        return result\n",
    "  \n",
    "    def attention_scoring(self, encoder_hidden_state, hidden_state):\n",
    "        N,_,H  = hidden_state.shape\n",
    "        attn_score = encoder_hidden_state.matmul(hidden_state.view(N,H,1))\n",
    "        attn_score = torch.softmax(attn_score, dim =1)\n",
    "        return attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default Setting\n",
    "h_dim = 128  # hidden_dimension\n",
    "input_size = len(word2index)  # Dimension of input meaning size of word2index\n",
    "output_size = len(word2index)\n",
    "emb_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn = Seq2Seq_Attention(input_size,input_size,h_dim,emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = self.df.iloc[idx, 0]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    itr = 1\n",
    "\n",
    "    train_dataset = MyDataset(all_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "\n",
    "  \n",
    "    for text, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoded_src = [tokenize(t, word2index) for t in text]\n",
    "        encoded_tgt = [tokenize_eng(t_, word2index) for t_ in label]\n",
    "\n",
    "        label = [e_t + [1] for e_t in encoded_tgt]\n",
    "        decoder_input = [[0]+e_t_+[1] for e_t_ in encoded_tgt]\n",
    "\n",
    "        encoded_src = torch.LongTensor(encoded_src).cuda()\n",
    "        decoder_input = torch.LongTensor(decoder_input).cuda()\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        outputs = model(encoded_src, decoder_input)\n",
    "        loss = criterion(outputs.transpose(1,2), label)\n",
    "        \n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if itr%20 == 0 :\n",
    "            sys.stdout.write('\\r epoch: %d, [iter: %d / all %d], Train Loss : %f' \\\n",
    "                          % (epoch, itr, len(train_loader), total_loss/total_len))\n",
    "            total_loss = 0\n",
    "            total_len = 0\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        itr+=1\n",
    "    now = time.gmtime(time.time() - start_time)\n",
    "    print('\\n')\n",
    "    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LG.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

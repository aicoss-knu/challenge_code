{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cheap-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "criminal-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/e9t/nsmc.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noted-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_table('./nsmc/ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 193518\n"
     ]
    }
   ],
   "source": [
    "#ALL\n",
    "all_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "all_data['document'] = all_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "all_data['document'] = all_data['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "all_data['document'].replace('', np.nan, inplace=True, regex=True) # 공백은 Null 값으로 변경\n",
    "all_data = all_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-elephant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>8963373</td>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3302770</td>\n",
       "      <td>쓰레기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>5458175</td>\n",
       "      <td>완전 사이코영화 마지막은 더욱더 이 영화의질을 떨어트린다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6908648</td>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>8548411</td>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193518 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1        8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2        4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3        9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4       10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1\n",
       "...          ...                                                ...    ...\n",
       "199995   8963373                                       포켓 몬스터 짜가 ㅡㅡ      0\n",
       "199996   3302770                                                쓰레기      0\n",
       "199997   5458175                    완전 사이코영화 마지막은 더욱더 이 영화의질을 떨어트린다      0\n",
       "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
       "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
       "\n",
       "[193518 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threatened-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "equipped-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193518/193518 [05:37<00:00, 573.44it/s]\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "X_data = []\n",
    "for sentence in tqdm(all_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "studied-wilderness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Make_Word2Index(ctxt : list):\n",
    "    \n",
    "    word2index = dict()\n",
    "    words = []\n",
    "    max_len = 0\n",
    "    for text in ctxt:\n",
    "        temp_len = len(text)\n",
    "        if max_len < temp_len:\n",
    "            max_len = temp_len\n",
    "        for word_temp in text:\n",
    "            words.append(word_temp)\n",
    "        \n",
    "    \n",
    "    word2index = {word : index for index, word in enumerate(list(set(words)))}\n",
    "    \n",
    "    return word2index, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, max_len = Make_Word2Index(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "differential-honor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49645"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "listed-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(text, word2index): # -> input = text, word2index from above function\n",
    "    one_hot_encode = []\n",
    "\n",
    "    tokenized_sentence = okt.morphs(text, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "\n",
    "    for word in stopwords_removed_sentence:\n",
    "        one_hot_vector = [0]*(len(word2index)+1)\n",
    "        try :\n",
    "            word2index[word]\n",
    "        except KeyError: \n",
    "            one_hot_vector[len(word2index)] = 1\n",
    "        else:\n",
    "            one_hot_vector[word2index[word]] = 1\n",
    "            one_hot_encode.append(one_hot_vector)\n",
    "\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rational-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(e : list):\n",
    "    padding = [0 for i in range(input_size)] \n",
    "    for i in range(max_len-len(e)):\n",
    "        e.append(padding)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transsexual-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Make LSTM Cell (class) \"\"\"\n",
    "\n",
    "### Use the above process to create a class\n",
    "\n",
    "class LSTM_Cell(nn.Module):\n",
    "  def __init__(self, input_size, h_dim):\n",
    "    super(LSTM_Cell, self).__init__() ### Inheritance, error if not present\n",
    "    self.x2h = nn.Linear(input_size, 4 * h_dim)\n",
    "    self.h2h = nn.Linear(h_dim, 4 * h_dim)\n",
    "\n",
    "  def forward(self, x, hidden_state, cell_state):\n",
    "    hx, cx = hidden_state, cell_state\n",
    "    all_gate = self.x2h(x) + self.h2h(hx)\n",
    "    input_gate, forget_gate, cell_gate ,output_gate = all_gate.chunk(4,dim=2)\n",
    "\n",
    "    input_gate = torch.sigmoid(input_gate)\n",
    "    forget_gate = torch.sigmoid(forget_gate)\n",
    "    cell_gate = torch.tanh(cell_gate)\n",
    "    output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "    new_cell_state = torch.mul(cx, forget_gate) + torch.mul(input_gate, cell_gate)\n",
    "    new_hidden_state = torch.mul(output_gate, torch.tanh(new_cell_state))    \n",
    "    \n",
    "    return new_cell_state, new_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "posted-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Make LSTM Cell with LSTM Cell (class) \"\"\"\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_size, h_dim, output_size):\n",
    "    super(LSTM,self).__init__()\n",
    "    self.h_dim = h_dim\n",
    "    self.lstm = LSTM_Cell(input_size, h_dim)\n",
    "    self.fc = nn.Linear(h_dim, output_size)\n",
    "  \n",
    "  def forward(self, x, hidden_state = None, cell_state = None): \n",
    "    if hidden_state is not None:\n",
    "      hidden_state = hidden_state\n",
    "    else:\n",
    "      hidden_state = torch.zeros((x.size(0),1,self.h_dim)).cuda()\n",
    "    if cell_state is not None:\n",
    "      cell_state = cell_state\n",
    "    else:\n",
    "      cell_state = torch.zeros((x.size(0),1,self.h_dim)).cuda()\n",
    "    \n",
    "    outputs = list() ## list for sum the outputs for each sequence\n",
    "\n",
    "    ## Use the for statement to proceed with the process for all sequence\n",
    "    for seq in range(x.size(1)) :\n",
    "      hidden_state, cell_state = self.lstm(x[:, seq, :].unsqueeze(1), hidden_state, cell_state)\n",
    "      outputs.append(hidden_state)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=1)\n",
    "    result = self.fc(outputs)\n",
    "\n",
    "    return result, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "owned-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default Setting\n",
    "h_dim = 128  # hidden_dimension\n",
    "input_size = len(word2index)+1  # Dimension of input meaning size of word2index\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incident-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "    def __init__(self, LSTM, input_size, h_dim, output_size):\n",
    "        super(Mymodel, self).__init__()\n",
    "\n",
    "        self.lstm = LSTM(input_size, h_dim, output_size)\n",
    "\n",
    "    def forward(self, sample, labels, hidden_state = None, cell_state = None):\n",
    "\n",
    "        outputs, hidden_state, cell_state = self.lstm(sample)\n",
    "        output = outputs[:][:,-1]\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        return loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "failing-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mymodel(LSTM, input_size, h_dim, output_size)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intensive-grass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 145393\n",
      "전처리 후 테스트용 샘플의 개수 : 48852\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./nsmc/ratings_train.txt', sep='\\t', quoting=3)\n",
    "test_df = pd.read_csv('./nsmc/ratings_test.txt', sep='\\t', quoting=3)\n",
    "\n",
    "#train_df\n",
    "train_df.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_df['document'] = train_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "train_df['document'] = train_df['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "train_df['document'].replace('', np.nan, inplace=True, regex=True) # 공백은 Null 값으로 변경\n",
    "train_df = train_df.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(train_df))\n",
    "\n",
    "#test_df\n",
    "test_df.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_df['document'] = test_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "test_df['document'] = test_df['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "test_df['document'].replace('', np.nan, inplace=True, regex=True) # 공백은 Null 값으로 변경\n",
    "test_df = test_df.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "graduate-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "british-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    itr = 1\n",
    "\n",
    "    train_dataset = MyDataset(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=8)\n",
    "\n",
    "  \n",
    "    for text, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoded_list = [one_hot_encoding(t, word2index) for t in text]\n",
    "        padded_list =  [zero_padding(e) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample = sample.type('torch.FloatTensor').cuda()\n",
    "        label = torch.tensor(label)\n",
    "        labels = label.clone().cuda()\n",
    "        \n",
    "        outputs = model(sample, labels)\n",
    "        loss, logits = outputs\n",
    "        \n",
    "        label_ = label.clone().cuda()\n",
    "        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        correct = pred.eq(label_)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label_)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        sys.stdout.write('\\r epoch: %d, [iter: %d / all %d], Train Loss : %f, Accyracy : %f' \\\n",
    "                          % (epoch, itr, len(train_loader), total_loss/total_len, total_correct/total_len))\n",
    "        total_loss = 0\n",
    "        total_len = 0\n",
    "        total_correct = 0\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        itr+=1\n",
    "    now = time.gmtime(time.time() - start_time)\n",
    "    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "valid-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    itr = 1\n",
    "\n",
    "    test_dataset = MyDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=40, shuffle=True, num_workers=8)\n",
    "\n",
    "  \n",
    "    for text, label in tqdm(test_loader):\n",
    "        \n",
    "        encoded_list = [one_hot_encoding(t, word2index) for t in text]\n",
    "        padded_list =  [zero_padding(e) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample = sample.type('torch.FloatTensor').cuda()\n",
    "        label = torch.tensor(label)\n",
    "        labels = label.clone().cuda()\n",
    "        \n",
    "        outputs = model(sample, labels)\n",
    "        loss, logits = outputs\n",
    "        \n",
    "        label_ = label.clone().cuda()\n",
    "        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        correct = pred.eq(label_)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label_)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "    print('Test Loss: {:.4f}, Accuracy: {:.3f}'.format(total_loss/total_len, total_correct/total_len))      \n",
    "    now = time.gmtime(time.time() - start_time)\n",
    "    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "banner-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
